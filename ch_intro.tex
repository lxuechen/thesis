\chapter{Introduction}

The issue of privacy in machine learning has gained considerable attention as an increasing amount of user data becomes a potential source for training predictive models.
Privacy attacks against machine learning pipelines have demonstrated that models trained without formal guarantees can reveal membership information and reproduce training data~\cite{shokri2017membership,carlini2021extracting}.
These vulnerabilities are not merely of academic interest---they have already led to societal harms and an increasing public distrust towards machine learning applications.
For instance, a South Korean company developed a chatbot based on (private) user data that regurgitated in its responses sensitive information such as nicknames and home addresses. This led to a public relations incident~\cite{sk-chatbot}.
In addition, privacy concerns impede the development of data-driven systems in specialized domains such as health care where a vast amount of quality data is sensitive.
These concerns present a challenge to fully realizing the societal benefits of AI and machine learning as they gain rising attention with tightening legislation and policy.

Differential Privacy~\cite[DP]{dwork2014algorithmic} is a formal privacy definition that has become the gold standard for statistical analysis with private data.
% Intuitively, a randomized algorithm satisfies DP if it is difficult (in a probabilistic sense) to distinguish the two outputs of the algorithm obtained on any two similar input datasets.
Differentially private algorithms are provably resistant to membership inference~\cite{yeom2018privacy,wasserman2010statistical} and data reconstruction attacks~\cite{pmlr-v162-guo22c,hayes2023bounding}.
Due to possessing a suite of attractive properties, DP algorithms have been deployed in high-stakes scenarios such as the 2020 US census~\cite{us-census} to protect users' privacy.

In contrast to the growing adoption of DP for private statistical data analysis, the privacy framework has seen limited industry adoption for deep learning applications.
This is because previous approaches for training deep learning models with the DP guarantee typically were computationally intensive or incurred substantial performance penalties on the resulting model.

This thesis contributes improved techniques for training deep learning models with DP that are much more efficient and performant than prior art.
These improvements have gained industry adoption and have made deep learning with DP a lot more compelling.
For example, some of the presented ideas led to the first set of deployments of differentially private machine learning at Microsoft.
% Chapter~\ref{ch_2} covers relevant background on privacy risks in machine learning, common DP algorithms for training models, and drawbacks of previous approaches.
% , building on ideas from past work~\cite{abadi2016deep,tramer2020differentially}.

The remaining thesis starts with Chapter~\ref{ch_2} which presents background on privacy risks in machine learning and common algorithms for model training with DP.
Chapter~\ref{ch_3} presents a gradient clipping technique that eliminates the memory overhead and approaches for improving the privacy-utility tradeoff.
Chapter~\ref{ch_4} presents techniques that eliminate the memory-compute tradeoff by extending ideas in the previous chapter.
Chapter~\ref{ch_5} provides theoretical interpretations of our empirical observations.
To make matters concrete, we summarize our contributions in Section~\ref{sec:overview_of_results} below.\\
% Our contributions are presented in Chapters~\ref{ch_3},\ref{ch_4}, and \ref{ch_5}; we summarize 

While most of the thesis works in a setting where private learning is aided with public data, it is worthwhile to note that certain ideas presented herein can be applied more broadly. 
For example, Chapter~\ref{ch_2} discusses how performant DP approaches can be potentially adopted to protect copyright when models are trained on copyrighted material.
Chapter~\ref{ch_3} discusses batch size tuning with effective noise, which can be applied to improve the performance of private pretraining.

Finally, we note that the general issue of privacy in machine learning is broad and complex.
While DP offers a robust and specific form of protection, it should not be seen as a universal solution to all issues related to privacy.
For example, DP does not address various privacy harms that occur during data collection~\cite{solove2005taxonomy}.
In addition, in order for DP algorithms to be effective at limiting the inference of participation, one needs to understand the context of the application as well as the underlying generation process of the private data~\cite{kifer2011no}. 
Chapter~\ref{ch_6} discusses the limitations and the broader societal impact in detail.

\section{Summary of Contributions}\label{sec:overview_of_results}



\newpage
\section{Summary of Publications}

\noindent The contents of Chapter~\ref{ch_3} consist of research ideas and results taken from:

\begin{mdframed}[leftline=true, topline=false, rightline=false, bottomline=false, linewidth=2pt]
Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. ``Large Language Models Can Be Strong Differentially Private Learners." International Conference on Learning Representations (ICLR), 2022.~\cite{li2022large}\\

\noindent Xiang Yue, Huseyin Inan, Xuechen Li, Girish Kumar, Julia McAnallen, Hoda Shajari, Huan Sun, David Levitan, and Robert Sim. ``Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe." Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), 2023.~\cite{yue-etal-2023-synthetic}
\end{mdframed}

\noindent The contents of Chapter~\ref{ch_4} consist of research ideas and results taken from:

\begin{mdframed}[leftline=true, topline=false, rightline=false, bottomline=false, linewidth=2pt]
Jiyan He*, Xuechen Li*, Da Yu*, Huishuai Zhang, Janardhan Kulkarni, Yin Tat Lee, Arturs Backurs, Nenghai Yu, and Jiang Bian. ``Exploring the Limits of Differentially Private Deep Learning With Group-Wise Clipping." International Conference on Learning Representations (ICLR), 2023.~\cite{he2022exploring}
\end{mdframed}

\noindent The contents of Chapter~\ref{ch_5} consist of research ideas and results taken from:

\begin{mdframed}[leftline=true, topline=false, rightline=false, bottomline=false, linewidth=2pt]
Xuechen Li*, Daogao Liu*, Tatsunori B. Hashimoto, Huseyin A. Inan, Janardhan Kulkarni, Yin-Tat Lee, and Abhradeep Guha Thakurta. ``When Does Differentially Private Learning Not Suffer in High Dimensions?" Advances in Neural Information Processing Systems (NeurIPS), 2022.~\cite{li2022does}
\end{mdframed}


\newpage
\paragraph{Non-thesis research.}
During my PhD, I have also contributed to the following research articles, which are excluded from this thesis.


\begin{verticalline}
Yann Dubois*, Xuechen Li*, Rohan Taori*, Tianyi Zhang*, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. ``AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback."
Advances in Neural Information Processing Systems (NeurIPS), 2023.~\cite{dubois2023alpacafarm}\\

\noindent
Rohan Taori*, Ishaan Gulrajani*, Tianyi Zhang*, Yann Dubois*, Xuechen Li*, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto.
``Stanford Alpaca: An Instruction-following LLaMA model.''~\cite{alpaca}\\

\noindent 
Peter Henderson*, Xuechen Li*, Dan Jurafsky, Tatsunori Hashimoto, Mark A. Lemley, and Percy Liang. 
``Foundation Models and Fair Use.''
Journal of Machine Learning Research (JMLR), 2023.~\cite{henderson2023fairuse}\\

\noindent 
Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. ``Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks."~\cite{kang2023exploiting}\\

\noindent 
Hanlin Zhang, Xuechen Li, Prithviraj Sen, Salim Roukos, and Tatsunori Hashimoto. 
``A Closer Look at the Calibration of Differentially Private Learners."~\cite{zhang2022closer}\\

\noindent 
Liang et al.
``Holistic Evaluation of Language Models.''
Transactions on Machine Learning Research (TMLR), 2023.~\cite{liang2023holistic}\\


\noindent 
Bommasani et al.
``On the Opportunities and Risks of Foundation Models.''~\cite{bommasani2021opportunities}

\end{verticalline}

\newpage
\section{Notation}
We use the following notation throughout.

\paragraph{Sets.} 
We reserve standard symbols for common sets.
Let $\mathbb{N}$ be the set of naturals (this includes 0), and $\mathbb{N}_+$ be the set of positive naturals (this excludes 0).
Let $\R$ be the set of reals.
For $n \in \mathbb{N}_+$, we define the shorthand $[n] = \{1, . . . , n\}$.
For a set $\mathcal{T}$, its size is represented by $|\mathcal{T}|$.

\paragraph{Linear Algebra.}
Given $d \in \mathbb{N}_+$, 
let $\mathbf{S}^d$ denote the set of $d \times d$ symmetric matrices, and $\mathbf{S}^d_+ \subset \mathbf{S}^d$ the positive semidefinite subset. 
Given $M \in \mathbf{S}^d$, let $\lambda_1(M) \ge \lambda_2(M) \ge \cdots \ge \lambda_d(M)$ denote its eigenvalues in descending order.

We use standard notation for various vector norms.
Let $v\in \R^d$ be a real vector with $d$ elements. 
We denote the Euclidean norm ($\ell^2$-norm) by $\|v\|_2$, 
the $\ell^\infty$-norm by $\|v\|_\infty$, and the $\ell^1$-norm by $\|v\|_1$.
Given $A \in \mathbf{S}^d_+$, let $\| x \|_A = (x^\top A x)^{1/2}$ denote the induced Mahalanobis norm.

\paragraph{Probability \& Statistics.}
We denote distributions using calligraphic letters, e.g., $\mathcal{D}, \mathcal{P}, \mathcal{Q}$. We use $x \sim \mathcal{D}$ to denote sampling an element $x$ from the distribution $\mathcal{D}$. For a random variable $X$ and event $O$, we denote the probability of the event as $\operatorname{Pr}[X\in O]$, the expectation of $X$ as $\mathbb{E}[X]$ and the variance as $\V[X]$. We denote the normal distribution with mean $\mu$ and standard deviation $\sigma$ as $\mathcal{N}\left(\mu, \sigma^2\right)$.

\paragraph{Miscellaneous.}
For scalar functions $f,g: \mathcal{X} \to \R$, we say $f \lesssim g$ if $f(x) \leq C g(x)$ for some numerical constant $C>0$ and all $x\in\mathcal{X}$. 
