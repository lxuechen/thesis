\chapter{Background}\label{ch_2}
\chaptermark{Background}

Privacy in machine learning has a multitude of meanings.
For example, a hospital may be interested in using an external machine learning service but is afraid that sending input queries with its patient data to the external party can breach privacy.
This is an example that necessitates \emph{private inference}.
On the other hand, a tech corporation may be interested in training better predictive models for their services with user data but is afraid that deploying such models would leak sensitive information in the user dataset.
This scenario calls for \emph{private training} which is the focus of this thesis. 

% motivation!
Private training has become ever more relevant for at least two reasons. 
First, as deep learning models become larger and optimization algorithms become better, the models we obtain tend to \emph{memorize} non-trivial fragments of the data on which they were trained.
This can result in models leaking membership information of data contributors under bespoke privacy attacks~\cite{shokri2017membership} or reproducing training examples simply when queried at deploy time~\cite{carlini2020extracting,nasr2023scalable}.
Second, many high-value and information-rich datasets contain sensitive information.
These datasets can range from private user datasets that capture interesting population-level statistics~\cite{google-2019}, or public web scrapes that include PIIs which got accidentally distributed on the public internet~\cite{carlini2020extracting}.
Private training is a tool to help model developers harvest the value of these datasets while minimizing the privacy risk due to model memorization.

Before studying approaches for private training, it is important to precisely define the privacy guarantee that we are after.
Thanks to the groundwork in the past decades, we now have the mature formal framework of \emph{Differential Privacy} to work with~\cite[DP]{dwork2006calibrating,dwork2014algorithmic}.
The remainder of this chapter reviews this framework and its adoption in deep learning.
We review basic definitions of DP, algorithms for training deep learning models with DP, and past research on deep learning with DP.
% as recent works demonstrated the potential for large models to leak membership information~\cite{shokri2017membership} and reproduce training data~\cite{carlini2020extracting}.

% Private training can be seen as a tool to harvest the value of private data (at a population level) while mitigating the risks of leaking 
% why is private training important?

% mention membership inference attacks shroki stuff, data reconstruction attacks carlini extracting text 2 papers, papernot privacy papers.


% Early works on private training studied simple models such as logistic regression models~\cite{chaudhuri2008privacy,chaudhuri2011differentially} and support vector machines~\cite{rubinstein2009learning}.
% Subsequent works studied private training for deep learning models~\cite{shokri2015privacy,abadi2016deep,papernot2016semi,papernot2018scalable}.

% Machine learning systems trained on sensitive user data can be vulnerable to privacy attacks~\citep{shokri2017membership,hayes2019logan}.
% This issue is especially pressing for recent applications of large language models, as these models are capable of memorizing and reconstructing sensitive examples contained in 
% the training data~\citep{DBLP:journals/corr/ZhangBHRV16, carlini2020extracting}.

% As a result of these concerns, there has been a large interest in developing
% methods that provide data privacy guarantees for large language models.
% The standard paradigm for providing such a guarantee in machine learning is \textit{Differential Privacy} (DP)~\citep{dwork2006calibrating,dwork2014algorithmic}.
% Unfortunately, DP learning has typically struggled to produce useful models when applied to large language models, resulting in models with either vacuous privacy guarantees \citep{dupuy2021efficient} or performance far below non-private baselines.
% This is widely attributed to the fact that the core primitive of \textit{Differentially Private Stochastic Gradient Descent} (DP-SGD)~\citep{song2013stochastic,bassily2014differentially,abadi2016deep} injects noise that must scale with the number of parameters, resulting in large noise levels for large language models \citep{yu2021not}.

% \Chen{this is too light on attacks.}

\section{Differential Privacy}

\begin{defi}[Approximate-DP]
A randomized algorithm $\M: \X \to \Y$ is ($\epsilon, \delta$)-differentially private if for all neighboring datasets $X, X'\in\mathcal{X}$ and all $Y \subset \Y$, 
$$
\Prob{\M(X) \in Y} \le \exp( \epsilon ) \Prob{ \M(X') \in Y } + \delta.
\label{eq:epsilon_delta_dp}
$$
\end{defi}
When $\delta=0$, the randomized algorithm $\M$ is said to satisfy \emph{pure differential privacy}.
The definition of ``neighboring'' could vary based on context. 
For this thesis, we say that two datasets $X$ and $X'$ are neighboring if one could be obtained from
another by including an additional example/record. 
In other words, $X$ and $X'$ always differ by one in terms of size.\footnote{
In other text, this is referred to as \emph{unbounded DP}~\cite{kifer2011no}. When ``neighboring'' is defined as two datasets of the same size differ by one element, it is referred to as \emph{bounded DP}.
Unbounded DP implies bounded DP with a different privacy leakage parameter.
The reverse direction generally is not true.
}

\Chen{mention post-processing property, and composition theorem. the first is needed to understand why DP-SGD prevents leakage. the second is need to help understand how this fits the iterative optimization paradigm.}


\section[Differentially Private Stochastic Gradient Descent]{\large Differentially Private Stochastic Gradient Descent}

DP learning typically relies on DP optimizers which privatize gradients before performing updates. 
The privatization step ensures that parameter updates leak limited information about training examples through their gradients.
Specifically, this step clips per-example gradients with a norm constraint $C$, and adds Gaussian noise $z\sim\mathcal{N}(0, C^2\sigma^2 I_p)$ to the sum of clipped gradients.
Here, $\sigma$ is the \textit{noise multiplier} determined from the privacy budget $(\epsilon, \delta)$, number of gradient updates $S$, and sampling rate $q=\tfrac{B}{N}$ for a batch size of $B$ and a dataset size of $N$.\footnote{Since we adopted the definition of ``neighboring'' based on addition/removal, the batch size here should be interpreted as the lot size~\citep{abadi2016deep} or, equivalently stated, the expected size of a batch drawn with Poisson sampling.}
Intuitively, clipping individual gradients ensures that each example has bounded influence on the parameter update, whereas noising the gradient prevents exact tracing of particular examples.
The noise being isotropic implies that larger models would experience heavier noise per update, as the norm of the $p$-dimensional Gaussian $\normtwo{z}$ scales as $C \sigma \sqrt{p}$.
This is widely believed to be the cause for DP optimization to perform poorly at training high-dimensional deep learning models~\citep{gautum14,yu2021not}.

We use DP-Adam throughout. DP-Adam works just like regular Adam~\citep{kingma2014adam} but performs updates and moment accumulation with privatized gradients. 
The gradient privatization part is the same as that performed in DP-SGD~\citep{song2013stochastic,abadi2016deep}.
To determine the noise multiplier, we account privacy through R\'enyi differential privacy (RDP)~\citep{mironov2017renyi,mironov2019r}.
For completeness, we include the pseudocode for DP-Adam below. 

\vspace{-2mm}
\begin{algorithm}[H]\label{algo:dp_adam}
\centering
\caption{DP-Adam}
\begin{algorithmic}[1]
  \State {\bfseries Input:}
  Data $\D=\{x_i\}_{i=1}^N$, learning rate $\eta$, noise multiplier $\sigma$, batch size $B$, Euclidean norm threshold for gradients $C$, epochs $E$, initial parameter vector $\theta_0\in\R^p$, initial moment estimates $m_0, v_0 \in \R^p$,
  exponential decay rates $\beta_1, \beta_2 \in \R$,
  avoid division-by-zero constant $\gamma\in \R$.

  \For{ $t \in [E \cdot \nicefrac{N}{B}]$ }

    \State Draw a batch $B_t$ via Poisson sampling; each element has probability $\nicefrac{B}{N}$ of being selected
    \For{ $x_i \in B_t$ }
        \State
        $
        g_t(x_i) \gets \nabla_{\theta_t} \mathcal{L}(x_i), \quad
        \tilde{g_t}(x_i) \gets g_t(x_i) \cdot \min(1, \nicefrac{C}{ \normtwo{g_t(x_i)} })
        $
    \EndFor
    \State $z_t \sim \mathcal{N}(0, \sigma^2 C^2 I_p)$
    \State $\bar{g_t} = \frac{1}{B} \bracks{ 
        \sum_{i=1}^N \tilde{g_t}(x_i) + z_t
    }$
    \State $\theta_{t + 1}, m_{t+1}, v_{t+1} \gets \text{AdamUpdate}(\theta_t, m_{t}, v_{t}, \bar{g_t}, \beta_1, \beta_2, \gamma)$
    \EndFor
\State \Return $\theta_{\nicefrac{TN}{B}}$
\end{algorithmic}
\end{algorithm}

\vspace{-6mm}
\begin{algorithm}[H]\label{algo:adam_update}
\centering
\caption{AdamUpdate}
\begin{algorithmic}[1]
  \State {\bfseries Input:}
  $\theta_t, m_t, v_t, \bar{g_t}, \beta_1, \beta_2, \gamma$ 
  \State
  $
  m_{t+1} \leftarrow
    \beta_{1} \cdot m_{t} + \left(1-\beta_{1}\right) \cdot \bar{g_t}, \quad 
  v_{t+1} \leftarrow
    \beta_{2} \cdot v_{t}+\left(1-\beta_{2}\right) \cdot \bar{g_t}^{2}
  $
  \State
  $
  \widehat{m}_{t+1} \leftarrow m_{t+1} /\left(1-\beta_{1}^{t}\right), \quad
  \widehat{v}_{t+1} \leftarrow v_{t+1} /\left(1-\beta_{2}^{t}\right)
  $
  \State
  $
  \theta_{t+1} \leftarrow \theta_{t}-\alpha \cdot \widehat{m}_{t+1} /\left(\sqrt{\widehat{v}_{t+1}}+\gamma\right)
  $
  \State  \Return $\theta_{t+1}, m_{t +1}, v_{t + 1}$
\end{algorithmic}
\end{algorithm}
\Chen{algo box should be with generic update. then mention privacy guarantee does not change if you alter the update rule so long as gradient privatization is performed.}


\section{Why Differential Privacy?}
\Chen{past work heuristic privacy without def'n all broken. neuracrypt, instahide, texthide, etc. go back to dwork's works on why you need to start with def'n. maybe mention some crypto stuff as well.}

\section{Past Work on Deep Learning with DP}
\Chen{give some historical context on the performance and runtime overhead.}
