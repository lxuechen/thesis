\chapter{Background}\label{ch_2}
\chaptermark{Background}

Privacy has a multitude of meanings in machine learning.
For example, a hospital may be interested in using an external machine learning service but is afraid that sending input queries with its patient data to the external party can breach privacy.
This is an example that necessitates \emph{private inference}~\cite{tramer2018slalom,h100_confidential_computing}.
On the other hand, a tech corporation may be interested in training better predictive models for their services with user data but is concerned that deploying such models would leak sensitive information in the training data.
This scenario calls for \emph{private training} which is the focus of this thesis. 

% motivation!
Private training has become ever more relevant for at least two reasons. 
First, as deep learning models become larger and optimization algorithms become better, our models tend to \emph{memorize} non-trivial fragments of the data on which they were trained~\cite{DBLP:journals/corr/ZhangBHRV16,feldman2020neural,feldman2020does,brown2021memorization}.
This can result in models leaking membership information of data contributors under bespoke privacy attacks~\cite{shokri2017membership} or reproducing training examples simply when queried during deployment~\cite{carlini2020extracting,nasr2023scalable}.
Second, many high-value and information-rich datasets contain sensitive information.
These datasets can range from private user data that contain interesting and general patterns~\cite{google-2019} to public web scrapes which include PIIs that were accidentally distributed on the public internet~\cite{carlini2020extracting}.
Private training serves as a tool to unlock the value of these datasets while minimizing the privacy risk due to data memorization.

Before studying approaches for private training, it is important to precisely define the privacy guarantee that we aim to achieve.
Thanks to the groundwork in the past decades, we now have the mature formal framework of \emph{Differential Privacy} (DP) to work with~\cite{dwork2006calibrating,dwork2014algorithmic}.
The remainder of this chapter reviews this framework and its adoption in deep learning.
We review definitions of DP, common algorithms for training deep learning models with DP, and challenges faced by past research.
% as recent works demonstrated the potential for large models to leak membership information~\cite{shokri2017membership} and reproduce training data~\cite{carlini2020extracting}.

% Private training can be seen as a tool to harvest the value of private data (at a population level) while mitigating the risks of leaking 
% why is private training important?

% mention membership inference attacks shroki stuff, data reconstruction attacks carlini extracting text 2 papers, papernot privacy papers.


% Early works on private training studied simple models such as logistic regression models~\cite{chaudhuri2008privacy,chaudhuri2011differentially} and support vector machines~\cite{rubinstein2009learning}.
% Subsequent works studied private training for deep learning models~\cite{shokri2015privacy,abadi2016deep,papernot2016semi,papernot2018scalable}.

% Machine learning systems trained on sensitive user data can be vulnerable to privacy attacks~\citep{shokri2017membership,hayes2019logan}.
% This issue is especially pressing for recent applications of large language models, as these models are capable of memorizing and reconstructing sensitive examples contained in 
% the training data~\citep{DBLP:journals/corr/ZhangBHRV16, carlini2020extracting}.

% As a result of these concerns, there has been a large interest in developing
% methods that provide data privacy guarantees for large language models.
% The standard paradigm for providing such a guarantee in machine learning is \textit{Differential Privacy} (DP)~\citep{dwork2006calibrating,dwork2014algorithmic}.
% Unfortunately, DP learning has typically struggled to produce useful models when applied to large language models, resulting in models with either vacuous privacy guarantees \citep{dupuy2021efficient} or performance far below non-private baselines.
% This is widely attributed to the fact that the core primitive of \textit{Differentially Private Stochastic Gradient Descent} (DP-SGD)~\citep{song2013stochastic,bassily2014differentially,abadi2016deep} injects noise that must scale with the number of parameters, resulting in large noise levels for large language models \citep{yu2021not}.

% \Chen{this is too light on attacks.}

\section{Differential Privacy}
The privacy goal of DP is that when releasing information about a sensitive dataset, anything that can be learned about an individual example from the released information can be learned without that individual example.
DP formalizes this intuitive goal with a probabilistic statement that views any release procedure as a \emph{randomized algorithm} operating on datasets.
DP is a property of release procedures (randomized algorithms) and not of released artifacts (algorithm outputs).
Below, we define $(\epsilon, \delta)$-DP.
\begin{defi}[$(\epsilon, \delta)$-Differential Privacy]
A randomized algorithm $\M: \X \to \Y$ is ($\epsilon, \delta$)-differentially private if for all neighboring datasets $X, X'\in\mathcal{X}$ and all $Y \subset \Y$, 
$$
\Prob{\M(X) \in Y} \le \exp( \epsilon ) \Prob{ \M(X') \in Y } + \delta.
\label{eq:epsilon_delta_dp}
$$
\end{defi}
In the above, $\epsilon \ge 0$ and $\delta \ge 0$ are the \emph{privacy parameters}.
A smaller $\epsilon$ (or $\delta$) will yield better privacy.
($\epsilon, \delta$)-DP is also known as approximate-DP.
When $\delta=0$, the randomized algorithm $\M$ is said to satisfy \emph{pure differential privacy}.

The definition of ``neighboring'' could vary based on the context. 
For this thesis, we say that two datasets $X, X' \in \mathcal{X}$ are neighboring if one could be obtained from the other through the addition of an example. 
This implies that $X$ and $X'$ always differ by one in terms of the size of the set.\footnote{
In other text, this is referred to as \emph{unbounded DP}~\cite{kifer2011no}. When ``neighboring'' is defined as two datasets of the same size differ by one element, it is referred to as \emph{bounded DP}.
Unbounded DP implies bounded DP with a different privacy leakage parameter.
The reverse direction generally is not true.
}

DP enjoys several appealing properties. 
We recap the relevant ones here for presentation completeness and direct the reader to classic textbooks for additional details~\cite{dwork2014algorithmic,vadhan2017complexity}.

\subsection{Post-Processing}
The first property we recall is \emph{post-processing}.
This property says that no (data-independent) operation on a DP algorithm's output can incur additional privacy leakage (measured by the privacy parameters) beyond the leakage of the original DP algorithm.
\begin{prop}[Post-Processing]
Let $\F: \X \to \Y$ be a randomized algorithm that is ($\epsilon, \delta$)-DP.
Let $\mathcal{G}: \Y \to \Y'$ be an arbitrary mapping. 
Then, the composed algorithm $\mathcal{G} \circ \F$ is ($\epsilon, \delta$)-DP.
\end{prop}
Post-processing holds regardless of whether $\mathcal{G}$ is deterministic or randomized---so long as it doesn't take in any information of the original private dataset~\cite{dwork2014algorithmic}.
In the machine learning context, post-processing ensures that no additional privacy can be leaked no matter how one queries a model that is trained with a DP algorithm.


\subsection{Adaptive Privacy Composition}
The next property we recall is \emph{privacy composition}.
The property says that the procedure of jointly releasing the outputs of individual DP algorithms is also DP.
Here, we consider the more general \emph{adaptive} case, where each algorithm takes in some outputs of others for producing its own output.
The privacy leakage of the joint release can be numerically tracked with a \emph{privacy composition theorem}.
Here, we present an adaptive composition theorem, where the privacy loss of the joint release grows radically in the number of releases $k$ (as opposed to linear with \emph{basic composition}~\cite{dwork2014algorithmic}).
\begin{theo}[Strong Composition~\cite{dwork2014algorithmic,kairouz2015composition}]
Let $\F_1: \X \to \Y_1, \F_2: \X \times \Y_1 \to \Y_2, \dots, \F_k: \X \times \Y_1 \times \cdots \times \Y_{k-1} \to \Y_k$ be randomized algorithms that are each $(\epsilon, \delta)$-DP.
Then, the joint release $\F: X \mapsto \bracks{ Y_1, \dots, Y_k }$ defined by  
\eq{
  Y_1 = \F_1(X), \quad Y_2 = \F_2(X, Y_1), \quad \dots , \quad Y_k = \F_k(X, Y_1, \dots, Y_{k-1})
}
is $(\widetilde{\epsilon}, \widetilde{\delta})$-DP, where
\eq{
  \widetilde{\epsilon} &= \epsilon \sqrt{2k\log(1/\delta')} + \epsilon k \frac{\exp(\epsilon) - 1}{ \exp(\epsilon) + 1}, \\
  \widetilde{\delta} &= \delta k + \delta',
  \label{eq:strong_composition}
}
for all $\delta' > 0$.
\end{theo}
When $\epsilon$ is small (the high privacy regime), the first term in $\widetilde{\epsilon}$ scales as $O(\epsilon \sqrt{k})$ and becomes the dominant factor of the privacy loss. 
This can be seen from the fact that the second term scales as $O(\epsilon^2 k)$ and is thus much smaller due to the quadratic dependence on $\epsilon$.\footnote{To study the asymptotics of the second term, recall the standard approximation $e^x - 1 \approx x$ for small $x$.}
% Roughly speaking, this makes the privacy leakage $\widetilde{\epsilon}$ grow almost as $O(\sqrt{k})$.

The formulae for $\widetilde{\epsilon}$ and $\widetilde{\delta}$ in \eqref{eq:strong_composition} are obtained with standard concentration bounds, and the values on the right-hand side are conservative estimates of the true privacy leakage.
To derive tighter estimates, past works have studied privacy composition under alternative privacy constructions (such as R\'enyi differential privacy~\cite{mironov2017renyi}) or numerical composition of trade-off functions~\cite{dong2019gaussian,gopi2021numerical,zhu2022optimal}.
We take advantage of the tightness of these privacy accounting techniques in this work but direct the reader to the original papers for a thorough treatment of their machinery.

Adaptive privacy composition is important for building iterative optimizers with a DP guarantee, as it reduces the problem of figuring out the overall privacy loss of the iterative procedure to that of figuring out the privacy loss of a single step.

\section[Differentially Private Stochastic Gradient Descent]{\large Differentially Private Stochastic Gradient Descent}
We review common algorithms for training deep learning models with DP, focusing on iterative first-order optimizers such as DP-SGD and its siblings.
The release procedure here is the training algorithm that takes in a (private) dataset and outputs weights of the trained model.
In many scenarios, models weights are not actually released by model developers---instead, they are gated behind an API service that offers query access.
But recall when the training algorithm is DP, by virtue of the post-processing property, no interaction with the trained model leaks privacy.

To design an iterative DP algorithm, it suffices to design a single step of the algorithm and rely on privacy composition for an overall privacy guarantee.
Typical first-order optimizers iterate over many steps of parameter update.
Each step has two high-level procedures: (1) computing the gradient of the loss, and (2) updating model weights based on the gradient.
(1) is a function of the private dataset and incurs a privacy loss.
But if (1) is DP, (2) incurs no privacy loss due to post-processing.
Thus, it suffices to ensure the gradient computation step is DP.
The \emph{Gaussian mechanism} is a DP algorithm that can serve this purpose.
Below, we define the concept of \emph{sensitivity}, present the Gaussian mechanism algorithm, and give conditions under which it is $(\epsilon, \delta)$-DP.
\begin{defi}[$\ell_2$ Sensitivity]
Let $\M: \X \to \R^p$ be a mapping with vector outputs.
Its $\ell_2$ sensitivity $\Delta_2 \M$ is defined by
\eq{
  \Delta_2 \M = \max_{\substack{ \mathrm{neighboring}\; X, X' \in \X } } \norm{\M(X) - \M(X') }_2.
}
\end{defi}
The sensitivity of a mapping characterizes how much its output changes once the input dataset changes slightly (by one example).
Intuitively, mappings with low sensitivity are less affected by the presence/absence of individual examples, and thus would be ``more private.''
However, having low sensitivity does not immediately imply the mapping satisfies the DP statement.\footnote{Here's an example of a low-sensitivity release procedure that is not private at all. Imagine a group of people each with a positive number below 0.1. Everyone in the group except Bob has a rational number. We aim to release the sum of a subset of these numbers. The sensitivity is 0.1 and thus small, but whether Bob was included in the subset can be easily inferred (Bob is included if the sum is irrational).}
This is why we need algorithms such as the Gaussian mechanism to \emph{privatize} mappings.
The Gaussian mechanism simply adds large noise vectors to high-sensitivity mappings and small noise vectors to low-sensitivity mappings.
Algorithm~\ref{algo:gauss} presents the pseudocode and Theorem~\ref{theo:gauss} gives its privacy guarantee.
\begin{algorithm}[H]
\centering
\caption{Gaussian Mechanism}
\begin{algorithmic}[1]
  \State {\bfseries Input:}
    Mapping $\M: \X \to \R^p$ with $\ell_2$ sensitivity $C$, input dataset $X$, privacy parameters $\epsilon, \delta$.
  \State $\sigma \gets  \sqrt{2 \log(1.25 / \delta) } / \epsilon $
  \State $z \sim \mathcal{N}(0, \sigma^2 C^2 I_p)$
  \State $m \gets \M(X) + z$
  \State \Return $m$
\end{algorithmic}
\label{algo:gauss}
\end{algorithm}

\begin{theo}[Gaussian Mechanism~\cite{dwork2014algorithmic}]
\label{theo:gauss}
Algorithm~\ref{algo:gauss} is $(\epsilon, \delta)$-DP.
\end{theo}

\begin{algorithm}[H]
\centering
\caption{Gaussian Mechanism}
\begin{algorithmic}[1]
  \State {\bfseries Input:}
    Mapping $\M: \X \to \R^p$ with $\ell_2$ sensitivity $C$, input dataset $X$, privacy parameters $\epsilon, \delta$.
  \State $\sigma \gets  \sqrt{2 \log(1.25 / \delta) } / \epsilon $
  \State $z \sim \mathcal{N}(0, \sigma^2 C^2 I_p)$
  \State $m \gets \M(X) + z$
  \State \Return $m$
\end{algorithmic}
\label{algo:gauss}
\end{algorithm}

% The algorithm takes in a set of vectors, clips each by a norm threshold, and returns the noisy average of the clipped vectors where the noise is an isotropic Gaussian. Algorithm~\ref{algo:gauss} contains the pseudocode.

% \Chen{sensitivity of a non-private algorithm -> Gaussian mechanism -> Gaussian mechanism for mean -> why is this useful for iterative optimization.}

 % requires iterative optimizers that have DP guarantees.
% The privatization step ensures that parameter updates leak limited information about training examples through their gradients.
% Specifically, this step clips per-example gradients with a norm constraint $C$, and adds Gaussian noise $z\sim\mathcal{N}(0, C^2\sigma^2 I_p)$ to the sum of clipped gradients.
% Here, $\sigma$ is the \textit{noise multiplier} determined from the privacy budget $(\epsilon, \delta)$, number of gradient updates $S$, and sampling rate $q=\tfrac{B}{N}$ for a batch size of $B$ and a dataset size of $N$.\footnote{Since we adopted the definition of ``neighboring'' based on addition/removal, the batch size here should be interpreted as the lot size~\citep{abadi2016deep} or, equivalently stated, the expected size of a batch drawn with Poisson sampling.}
% Intuitively, clipping individual gradients ensures that each example has bounded influence on the parameter update, whereas noising the gradient prevents exact tracing of particular examples.
% The noise being isotropic implies that larger models would experience heavier noise per update, as the norm of the $p$-dimensional Gaussian $\normtwo{z}$ scales as $C \sigma \sqrt{p}$.
% This is widely believed to be the cause for DP optimization to perform poorly at training high-dimensional deep learning models~\citep{gautum14,yu2021not}.

% We use DP-Adam throughout. DP-Adam works just like regular Adam~\citep{kingma2014adam} but performs updates and moment accumulation with privatized gradients. 
% The gradient privatization part is the same as that performed in DP-SGD~\citep{song2013stochastic,abadi2016deep}.
% To determine the noise multiplier, we account privacy through R\'enyi differential privacy (RDP)~\citep{mironov2017renyi,mironov2019r}.
% For completeness, we include the pseudocode for DP-Adam below. 

DP-SGD and its siblings follow the paradigm described above: They apply the Gaussian mechanism to privatize each gradient computation step, rely on post-processing for the privacy guarantee for weight updates, and resort to adaptive composition for the overall privacy guarantee. 
Algorithm~\ref{algo:dp_first_order} makes this precise in pseudocode.
\begin{algorithm}[H]
\centering
\caption{DP First-Order Optimization}
\begin{algorithmic}[1]
  \State {\bfseries Input:}
  Dataset $D=\{x_i\}_{i=1}^N$, learning rate $\eta$, batch size $B$, norm threshold $C > 0$, training epochs $E$, initial weights $\theta_0\in\R^p$, optimizer state $s_0$, optimizer hyperparameters $H$, privacy parameters $\epsilon, \delta > 0$.

  \State $q = \nicefrac{B}{N}$ \Comment{ {\color{red} Derive the sampling rate.} }
  \State $T = \floor{ \nicefrac{E}{q} }$ \Comment{ {\color{red} Derive the total iteration count.}}
  \State Calculate the noise multiplier $\sigma$ from $\epsilon, \delta, q, E$ with adaptive composition and privacy amplification theorems.
  \For{ $t = 1, \dots, T $ }
    \State Draw a batch $B_t$ via Poisson sampling each element with probability $q$
    \For{ $x_i \in B_t$ }
        \State $g_t(x_i) \gets \nabla_{\theta_t} \mathcal{L}(x_i)$ \Comment{ {\color{red} Compute per-example gradient.}}
        \State $\tilde{g_t}(x_i) \gets g_t(x_i) \cdot \min(1, \nicefrac{C}{ \normtwo{g_t(x_i)} })$ \Comment{
        {\color{red} Clip per-example gradient.}}
    \EndFor
    \State \(\triangleright\) {\color{red} Estimate the average gradient with the sampled Gaussian mechanism.} 
    \State $z_t \sim \mathcal{N}(0, \sigma^2 C^2 I_p)$
    \State $\bar{g_t} = \frac{1}{B} \bracks{ 
        \sum_{x_i \in B_t} \tilde{g_t}(x_i) + z_t
    }$
    \State \(\triangleright\) {\color{red} Update model weights and optimizer state.}
    \State $\theta_{t + 1}, s_{t+1} \gets \texttt{WeightUpdate}(\theta_t, s_t, H)$
    \EndFor
\State \Return $\theta_{T}$ or $\frac{1}{T}\sum_{t=1}^T \theta_t$
\end{algorithmic}
\label{algo:dp_first_order}
\end{algorithm}

% \begin{algorithm}[H]\label{algo:adam_update}
% \centering
% \caption{AdamUpdate}
% \begin{algorithmic}[1]
%   \State {\bfseries Input:}
%   $\theta_t, m_t, v_t, \bar{g_t}, \beta_1, \beta_2, \gamma$ 
%   \State
%   $
%   m_{t+1} \leftarrow
%     \beta_{1} \cdot m_{t} + \left(1-\beta_{1}\right) \cdot \bar{g_t}, \quad 
%   v_{t+1} \leftarrow
%     \beta_{2} \cdot v_{t}+\left(1-\beta_{2}\right) \cdot \bar{g_t}^{2}
%   $
%   \State
%   $
%   \widehat{m}_{t+1} \leftarrow m_{t+1} /\left(1-\beta_{1}^{t}\right), \quad
%   \widehat{v}_{t+1} \leftarrow v_{t+1} /\left(1-\beta_{2}^{t}\right)
%   $
%   \State
%   $
%   \theta_{t+1} \leftarrow \theta_{t}-\alpha \cdot \widehat{m}_{t+1} /\left(\sqrt{\widehat{v}_{t+1}}+\gamma\right)
%   $
%   \State  \Return $\theta_{t+1}, m_{t +1}, v_{t + 1}$
% \end{algorithmic}
% \end{algorithm}

% \Chen{algo box should be with generic update. then mention privacy guarantee does not change if you alter the update rule so long as gradient privatization is performed.}


\section{Why Differential Privacy?}
% \Chen{past work heuristic privacy without def'n all broken. neuracrypt, instahide, texthide, etc. go back to dwork's works on why you need to start with def'n. maybe mention some crypto stuff as well.}

\section{Challenges to Deep Learning with DP}
% \Chen{give some historical context on the performance and runtime overhead.}
