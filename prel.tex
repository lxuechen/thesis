\section{Preliminaries}
We define the notation used throughout this work and state the problems of differentially private empirical risk minimization and differentially private stochastic convex optimization.
Finally, we give a brief recap of differentially private stochastic gradient descent, and existing dimension-dependent and dimension-independent results in the literature.

\paragraph{Notation \& Terminology.} 
For a positive integer $n \in \mathbb{N}_+$, define the shorthand $[n] = \{1, \dots, n\}$.
For a vector $x \in \R^d$, denote its $\ell_2$-norm by $\normtwo{x}$.
Given a symmetric $M \in \R^{d \times d}$, let $\lambda_1(M) \ge \lambda_2(M) \ge \cdots \ge \lambda_d(M)$ denote its eigenvalues.
Given a positive semidefinite matrix $A$, let $\| x \|_A = (x^\top A x)^{1/2}$ denote the induced Mahalanobis norm.
For scalar functions $f$ and $g$, we write $f \lesssim g$ if there exists a positive constant $C$ such that $f(x) \leq C g(x)$ for all input $x$ in the domain.

\subsection{Differentially Private Empirical Risk Minimization and Stochastic Convex Optimization}
Before stating the theoretical problem of interest, we recall the basic concepts of Lipschitz continuity, convexity.
\begin{defi}[Lipschitz Continuity]
The loss function $h:\cK\to\R$ is $G$-Lipschitz with respect to the $\ell_2$ norm if for all $x,x' \in \cK$, $|f(x)-f(x')|\leq G\|x-x'\|_2$.
\end{defi}

\begin{defi}[Convexity]
The loss function $h:\cK\to\R$ is convex if $h(\alpha x + (1 - \alpha) y) \le \alpha h(x) + (1 - \alpha) h(y)$, for all $\alpha \in [0, 1]$, and $x, y$ in a convex domain $\cK$.
\end{defi}

% \begin{defi}[Approximate Differential Privacy~\cite{dwork2014algorithmic}]
% A randomized algorithm $\cM$ is $(\epsilon,\delta)$-differentially private if for all neighboring datasets $\cD$ and $\cD'$ that differ by a single record and all sets $\cO\subset\mathrm{range}(\cM)$, the following expression holds
% \begin{align*}
%     \Pr[\cM(\cD)\in \cO]\le \exp ({\epsilon}) \Pr[\cM(\cD')\in \cO]+\delta.
% \end{align*}
% \end{defi}
In this work, we study both \emph{differentially private empirical risk minimization} (DP-ERM) for convex objectives and \emph{differentially private stochastic convex optimization} (DP-SCO).

In DP-ERM for convex objectives, we are given a dataset $\cD=\{s_{j}\}_{j\in[n]} \in \mathcal{S}^n$ of $n$ examples. 
Each per-example loss $f(\cdot ; s_j)$ is convex over the convex body $\cK\subseteq\R^{d}$ and $G$-Lipschitz.
We aim to design an $(\epsilon,\delta)$-DP algorithm that returns a solution $x^\text{priv}$ which approximately minimizes the empirical risk $F(x;\cD):=\frac{1}{n}\sum_{s_{j}\in \cD}f(x;s_{j})$.
The term $\mathbb{E}_{x^\text{priv}} \sbracks{ F(x^\text{priv};\cD)-\min_{x\in\cK}F(x;\cD) }$ is referred to as the \emph{excess empirical risk}.

In DP-SCO, we assume the per-example loss $f(\cdot;s)$ is convex and $G$-Lipschitz for all $s\in \mathcal{S}$, and we are given $n$ examples drawn i.i.d. from some (unknown) distribution $\cP$.
The goal is to design an $(\epsilon,\delta)$-DP algorithm which approximately minimizes the population risk
$F(x;\cP):=\E_{s\sim\cP} [ f(x;s) ]$.
The term $\mathbb{E}_{x^\text{priv}} \sbracks{ F(x^\text{priv} ; \cP)-\min_{x\in\cK}F(x;\cP) }$ is referred to as the \emph{excess population risk}.

\subsection{Differentially Private Stochastic Gradient Descent}
\emph{Differentially Private Stochastic Gradient Descent} (DP-SGD)~\citep{abadi2016deep,song2013stochastic} is a popular algorithm for DP convex optimization.
For the theoretical analysis, we study DP-SGD for \emph{unconstrained} optimization. 
To facilitate analysis, we work with the $\ell_{2}$ regularized objective expressed as 
\eqn{
    F_\alpha (x; \cD) = \frac{1}{n} \sum_{j=1}^{n} f(x ; s_j) + \frac{\alpha}{2} \| x - x^{(0)} \|_2^2. 
}
To optimize this objective, DP-SGD independently samples an example in each iteration and updates the solution by combining the gradient with an isotropic Gaussian whose scale is proportional to $G$, the Lipschitz constant of $f$. 
Algorithm \ref{alg:noisy_SGD} presents the pseudocode.
% \begin{algorithm2e}
% \caption{DP-SGD for optimizing regularized finite-sum objectives}
% \label{alg:noisy_SGD}
% {\bf Input:} Initial iterate $x^{(0)}$, dataset $\cD = \{s_j\}_{j \in [n]}$, per-step noise magnitude $\sigma$, number of updates $T$, learning rate $\eta$, Lipschitz constant $G$ of $f$. \\
% \For{$t=1,\ldots,T$}
% {
% $j_t \sim \text{Uniform}([n])$\\
% $x^{(t)} = 
%     x^{(t-1)} -
%     \eta\Big(\nabla f(x^{(t-1)};s_{j_t}) + 
%     \alpha(x^{(t-1)}-x^{(0)})+ G \cdot \zeta_t \Big), \quad \zeta_t \sim \cN(0,\sigma^{2}I_d)$
% }
% {\bf Return:} $\overline{x}\defeq\frac{1}{T}\sum_{t=1}^{T}x^{(t)}$.
% \end{algorithm2e}
\begin{algorithm}
\caption{DP-SGD for Optimizing Regularized Finite-Sum Objectives}
\label{alg:noisy_SGD}
\begin{algorithmic}[1]
\State {\bfseries Input:} Initial iterate $x^{(0)}$, dataset $\cD = \{s_j\}_{j \in [n]}$, per-step noise magnitude $\sigma$, number of updates $T$, learning rate $\eta$, Lipschitz constant $G$ of $f$.
\For{$t=1,\ldots,T$}
    \State $j_t \sim \text{Uniform}([n])$
    \State $x^{(t)} = x^{(t-1)} - \eta\Big(\nabla f(x^{(t-1)};s_{j_t}) + \alpha(x^{(t-1)}-x^{(0)})+ G \cdot \zeta_t \Big), \quad \zeta_t \sim \cN(0,\sigma^{2}I_d)$
\EndFor
\State \Return $\overline{x}\defeq\frac{1}{T}\sum_{t=1}^{T}x^{(t)}$.
\end{algorithmic}
\end{algorithm}


It is straightforward to show that Algorithm~\ref{alg:noisy_SGD} satisfies differential privacy. 
The following lemma quantifies the overall privacy spending and builds on a long line of work on accounting the privacy loss of compositions~\citep{abadi2016deep,balle2018privacy}.
\begin{lemm}[\citep{KLL21}]
\label{lm:privacy_guarantee}
There exists constants $c_1$ and $c_2$ such that for $n\geq10$, $\epsilon< c_1 T/n^2$ and $\delta \in (0, \frac{1}{2}]$, DP-SGD (Algorithm~\ref{alg:noisy_SGD}) is $(\epsilon,\delta)$-DP whenever $\sigma \ge \frac{c_2 \sqrt{T\log(1/\delta)}}{\epsilon n}$.
\end{lemm}

\subsection{On the Dimension Dependence of Private Learning}
Early works on bounding the excess empirical and population risks for privately optimizing convex objectives focused on a constrained optimization setup where algorithms typically project iterates back onto a fixed bounded domain after each noisy gradient update. 
Results in this setting suggested that risks are inevitably dimension-dependent in the worst case.
For instance, it was shown that the excess empirical risk bound $\Theta (  G \normtwo{\cK} \sqrt{d\log(1/\delta)} n^{-1} \epsilon^{-1} )$ and excess population risk bound $\Theta ( G \normtwo{\cK} ( n^{-1/2} + \sqrt {d \log(1/\delta) } n^{-1} \epsilon^{-1} ) )$ are tight when privately optimizing convex $G$-Lipschitz objectives in a convex domain of diameter $\normtwo{\cK}$~\citep{bassily2014private}.
Moreover, the lower bound instances in these works imply that such dimension-dependent lower bounds also apply when one considers the class of loss functions whose gradients are low-rank.

The body of work on unconstrained convex optimization is arguably less abundant, with the notable result that differentially private gradient descent need not suffer from a dimension-dependent penalty when learning generalized linear models with low-rank data (equivalently stated, when gradients are low-rank)~\citep{song2021evading}.
Our main theoretical results (Theorems~\ref{thm:DPERM} and~\ref{thm:DPSCO}) extend this line of work and show that dimension-independence is achievable under weaker conditions.
% \Daogao{The last sentence somehow makes me feel we just make an independent/different assumption compared the one in \cite{song2021evading} and waters down the importance of our results.}

% However, recent empirical findings in large-scale language model fine-tuning demonstrated that very large models can be optimized with DP-SGD to achieve performance approaching that of non-private models~\cite{li2021large,yu2021differentially}. 
% Moreover, fine-tuning more parameters (and hence injecting more noise during training) does not necessarily hurt private fine-tuning performance~\cite{li2021large}.
% These empirical observations suggested that the error of learning does not degrade substantially (or sometimes negligibly) with the increase in problem dimension, pointing towards a seeming conflict between theory and practice.

% In the following, we set out to reconcile this discrepancy. 
% Section~\ref{sec:theory} shows that better error dimension-dependence can be achieved for private convex optimization if the objective satisfies additional regularity conditions in addition to Lipschitz continuity.
% Section \ref{sec:experiments} verifies our theory and presents initial evidence that private language model fine-tuning exhibits the type of regularity that leads to dimension-independent bounds in our theoretical results.
