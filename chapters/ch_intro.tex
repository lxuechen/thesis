\chapter{Introduction}

The issue of privacy in machine learning has gained considerable attention as an increasing amount of user data becomes a potential source for training predictive models.
Privacy attacks against machine learning pipelines have demonstrated that models trained without formal guarantees can reveal membership information and reproduce training data~\cite{shokri2017membership,carlini2021extracting}.
These vulnerabilities are not merely of academic interest---they have already led to societal harms and an increasing public distrust towards machine learning applications.
For instance, a South Korean company developed a chatbot with (private) user data that regurgitated sensitive information such as nicknames and home addresses. This led to a public relations incident~\cite{sk-chatbot}.
In addition, privacy concerns impede the development of data-driven systems in specialized domains such as health care where a vast amount of quality data is sensitive.
These concerns present a challenge to fully realizing the societal benefits of machine learning and AI as they gain rising attention with tightening legislation and policy.

Differential Privacy~\cite[DP]{dwork2014algorithmic} is a formal privacy definition that has become the gold standard for statistical analysis with private data.
% Intuitively, a randomized algorithm satisfies DP if it is difficult (in a probabilistic sense) to distinguish the two outputs of the algorithm obtained on any two similar input datasets.
Differentially private algorithms are provably resistant to membership inference~\cite{yeom2018privacy,wasserman2010statistical} and data reconstruction attacks~\cite{pmlr-v162-guo22c,hayes2023bounding}.
As such, these algorithms have been deployed in high-stakes scenarios such as the 2020 US census~\cite{us-census} to protect users' privacy.

In contrast to the growing adoption of DP for private statistical data analysis, the privacy framework has seen limited industry adoption for deep learning applications.
This is because previous approaches for training deep learning models with DP guarantees typically were computationally intensive or incurred substantial performance penalties on the resulting model.

This thesis contributes improved techniques for training deep learning models with DP that are much more efficient and performant than prior art.
These improvements have gained industry adoption and have made deep learning with DP compelling.
For example, some of the presented ideas led to the first set of deployments of differentially private machine learning at Microsoft.
% Chapter~\ref{ch_2} covers relevant background on privacy risks in machine learning, common DP algorithms for training models, and drawbacks of previous approaches.
% , building on ideas from past work~\cite{abadi2016deep,tramer2020differentially}.

The remaining thesis starts with Chapter~\ref{ch_2} which presents background on privacy risks in machine learning and common algorithms for training models with DP.
Chapter~\ref{ch_3} presents a technique that eliminates the memory overhead of training with DP and approaches for improving the privacy-utility tradeoff.
Chapter~\ref{ch_4} presents techniques that further eliminate the memory-compute tradeoff by extending ideas in the previous chapter.
Chapter~\ref{ch_5} provides theoretical interpretations of our empirical observations.
We summarize these contributions in Section~\ref{sec:overview_of_results} below.\\
% Our contributions are presented in Chapters~\ref{ch_3},\ref{ch_4}, and \ref{ch_5}; we summarize 

While most of the thesis works in a setting where private learning is aided with public data, it is worthwhile to note that certain ideas presented herein can be applied more broadly. 
For example, Chapter~\ref{ch_2} discusses how performant DP approaches can be useful to protect copyright when models are trained on copyrighted material.
Chapter~\ref{ch_3} discusses batch size tuning with effective noise, which can be applied to improve the performance of private pretraining.

Finally, we note that the general issue of privacy in machine learning is broad and complex.
While DP offers a robust and specific form of protection, it should not be seen as a universal solution to all issues related to privacy.
For example, DP does not address various privacy harms that occur during data collection~\cite{solove2005taxonomy}.
In addition, in order for DP algorithms to be effective at limiting the \emph{inference of participation}, one needs to understand the context of the application as well as the underlying generation process of the private data~\cite{kifer2011no}. 
Chapter~\ref{ch_6} discusses the limitations and the broader societal impact in detail.

\section{Summary of Contributions}\label{sec:overview_of_results}

Before summarizing our contributions, we recap the \emph{Differentially Private Stochastic Gradient Descent} (DP-SGD) algorithm.
DP-SGD is a popular algorithm for training deep learning models with DP owing to its algorithmic simplicity and its ease of privacy accounting through standard composition theorems~\cite{dwork2014algorithmic}.
Similar to plain SGD, DP-SGD updates model weights incrementally with the gradient computed at each iteration.
On the other hand, DP-SGD differs from SGD in that it clips \emph{per-example gradients} and noises their average before updating the weights for each iteration. 
For the purpose of this chapter, we summarize the challenges to making DP-SGD effective in practice below and defer to Chapter~\ref{ch_2} for a detailed treatment of the algorithm. 

\paragraph{DP-SGD is computationally intensive.} DP-SGD is known to be computationally costly due to clipping per-example gradients.
Instantiating per-example gradients and normalizing them can incur both high memory and time costs in standard machine learning frameworks~\citep{paszke2019pytorch,frostig2018compiling}, and thus private machine learning with DP-SGD is reportedly much more memory demanding and/or slower than its non-private counterpart~\citep{carlini2019secret,hoory2021learning}. 

\paragraph{DP-SGD incurs performance loss.}
DP-SGD is known to (potentially) incur substantial performance losses in the resulting model compared to SGD due to the isotropic privacy noise added to parameter updates in each iteration.\\

\subsection{Contributions}

Chapter~\ref{ch_3} addresses the memory and performance challenges of DP-SGD.
In particular, the chapter first presents \emph{ghost clipping}, a memory-saving technique that makes training with DP as memory efficient as standard training for Transformer models---with a modest wall time overhead.
To tackle the performance issue, the chapter provides prescriptions for hyperparameter tuning and shows that fine-tuning pretrained language models with DP-SGD yields strong performance for a suite of NLP tasks at common privacy levels.
In the end, the chapter presents empirical observations that the dimensionality of gradient updates does not explain private fine-tuning performance---contrary to conventional wisdom.
For example, larger pretrained models lead to better private fine-tuning results, and parameter-efficient adaptation methods with a reduced dimensionality of updates do not necessarily outperform a baseline method that fine-tunes all model parameters.

Chapter~\ref{ch_4} addresses the run time overhead DP-SGD, leveraging
two instantiations of \emph{group-wise clipping}.
The chapter first presents a particular implementation of \emph{per-layer clipping}, which clips layer-wise per-example gradients \emph{in conjunction} with backpropagation in DP-SGD. 
This results in DP training that is as memory-efficient and (almost) as fast per update as standard training for many small- to moderate-scale workflows of interest.
The remainder of the chapter presents \emph{per-device clipping}, which eliminates communication overheads for large-scale DP training workflows where model sharding is necessary.
With this approach, we scale DP fine-tuning to work with the 175 billion-parameter GPT-3 and obtain a state-of-the-art task performance for a challenging summarization task.


Chapter~\ref{ch_5} studies when DP-SGD performs well for optimizing high dimensional objectives. 
This chapter is motivated by the goal to reconcile the seeming discrepancy between the conventional wisdom (DP-SGD yields worse bounds with increasing dimension) and our empirical observations in Chapter~\ref{ch_3} (dimensionality does not explain performance).
% We aim to provide a better understanding of the relation between problem dimension and the performance of differentially private learning.
On the theoretical front, we show that DP-SGD can result in dimension-independent error bounds even when gradients span the entire ambient space for unconstrained optimization problems.
We formalize a new condition termed \emph{Restricted Lipschitz Continuity} and derive refined bounds for the excess empirical and population risks for DP-SGD when convex loss functions obey this condition.
We empirically show that gradients of language models during fine-tuning are mostly
controlled by a few principal components---a behavior that is similar to conditions under which we
obtain dimension-independent bounds. 
This provides a possible explanation for the observation that densely fine-tuning with DP-SGD need not necessarily experience much
worse performance than sparsely fine-tuning.

\newpage
\section{Summary of Publications}

\noindent The contents of Chapter~\ref{ch_3} consist of research ideas and results taken from:

\begin{mdframed}[leftline=true, topline=false, rightline=false, bottomline=false, linewidth=2pt]
Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. ``Large Language Models Can Be Strong Differentially Private Learners." International Conference on Learning Representations (ICLR), 2022.~\cite{li2022large}
\end{mdframed}

\noindent The contents of Chapter~\ref{ch_4} consist of research ideas and results taken from:

\begin{mdframed}[leftline=true, topline=false, rightline=false, bottomline=false, linewidth=2pt]
Jiyan He*, Xuechen Li*, Da Yu*, Huishuai Zhang, Janardhan Kulkarni, Yin Tat Lee, Arturs Backurs, Nenghai Yu, and Jiang Bian. ``Exploring the Limits of Differentially Private Deep Learning With Group-Wise Clipping." International Conference on Learning Representations (ICLR), 2023.~\cite{he2022exploring}
\end{mdframed}

\noindent The contents of Chapter~\ref{ch_5} consist of research ideas and results taken from:

\begin{mdframed}[leftline=true, topline=false, rightline=false, bottomline=false, linewidth=2pt]
Xuechen Li*, Daogao Liu*, Tatsunori B. Hashimoto, Huseyin A. Inan, Janardhan Kulkarni, Yin-Tat Lee, and Abhradeep Guha Thakurta. ``When Does Differentially Private Learning Not Suffer in High Dimensions?" Advances in Neural Information Processing Systems (NeurIPS), 2022.~\cite{li2022does}
\end{mdframed}


\newpage
\paragraph{Non-thesis research.}
During my PhD, I have also contributed to the following research articles, which are excluded from this thesis.


\begin{verticalline}
Yann Dubois*, Xuechen Li*, Rohan Taori*, Tianyi Zhang*, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. ``AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback."
Advances in Neural Information Processing Systems (NeurIPS), 2023.~\cite{dubois2023alpacafarm}\\

\noindent
Rohan Taori*, Ishaan Gulrajani*, Tianyi Zhang*, Yann Dubois*, Xuechen Li*, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto.
``Stanford Alpaca: An Instruction-following LLaMA model.''~\cite{alpaca}\\

\noindent 
Peter Henderson*, Xuechen Li*, Dan Jurafsky, Tatsunori Hashimoto, Mark A. Lemley, and Percy Liang. 
``Foundation Models and Fair Use.''
Journal of Machine Learning Research (JMLR), 2023.~\cite{henderson2023fairuse}\\

\noindent Xiang Yue, Huseyin Inan, Xuechen Li, Girish Kumar, Julia McAnallen, Hoda Shajari, Huan Sun, David Levitan, and Robert Sim. ``Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe." Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), 2023.~\cite{yue-etal-2023-synthetic}\\

\noindent 
Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. ``Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks."~\cite{kang2023exploiting}\\

\noindent 
Hanlin Zhang, Xuechen Li, Prithviraj Sen, Salim Roukos, and Tatsunori Hashimoto. 
``A Closer Look at the Calibration of Differentially Private Learners."~\cite{zhang2022closer}\\

\noindent 
Liang et al.
``Holistic Evaluation of Language Models.''
Transactions on Machine Learning Research (TMLR), 2023.~\cite{liang2023holistic}\\

\noindent 
Bommasani et al.
``On the Opportunities and Risks of Foundation Models.''~\cite{bommasani2021opportunities}

\end{verticalline}

\newpage
\section{Notation}
We use the following notation throughout.

\paragraph{Sets.} 
We reserve standard symbols for common sets.
Let $\mathbb{N}$ be the set of naturals (this includes 0), and $\mathbb{N}_+$ be the set of positive naturals (this excludes 0).
Let $\R$ be the set of reals.
For $n \in \mathbb{N}_+$, we define the shorthand $[n] = \{1, . . . , n\}$.
For a set $\mathcal{T}$, its size is represented by $|\mathcal{T}|$.

\paragraph{Linear Algebra.}
Given $d \in \mathbb{N}_+$, 
let $\mathbf{S}^d$ denote the set of $d \times d$ symmetric matrices, and $\mathbf{S}^d_+ \subset \mathbf{S}^d$ the positive semidefinite subset. 
Given $M \in \mathbf{S}^d$, let $\lambda_1(M) \ge \lambda_2(M) \ge \cdots \ge \lambda_d(M)$ denote its eigenvalues in descending order.

We use standard notation for various vector norms.
Let $v\in \R^d$ be a real vector with $d$ elements. 
We denote the Euclidean norm ($\ell^2$-norm) by $\|v\|_2$, 
the $\ell^\infty$-norm by $\|v\|_\infty$, and the $\ell^1$-norm by $\|v\|_1$.
Given $A \in \mathbf{S}^d_+$, let $\| x \|_A = (x^\top A x)^{1/2}$ denote the induced Mahalanobis norm.

\paragraph{Probability \& Statistics.}
We denote distributions using calligraphic letters, e.g., $\mathcal{D}, \mathcal{P}, \mathcal{Q}$. We use $x \sim \mathcal{D}$ to denote sampling an element $x$ from the distribution $\mathcal{D}$. For a random variable $X$ and event $O$, we denote the probability of the event as $\operatorname{Pr}[X\in O]$, the expectation of $X$ as $\mathbb{E}[X]$ and the variance as $\V[X]$. We denote the normal distribution with mean $\mu$ and standard deviation $\sigma$ as $\mathcal{N}\left(\mu, \sigma^2\right)$.

\paragraph{Miscellaneous.}
For scalar functions $f,g: \mathcal{X} \to \R$, we say $f \lesssim g$ if $f(x) \leq C g(x)$ for some numerical constant $C>0$ and all $x\in\mathcal{X}$. 
